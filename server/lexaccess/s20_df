diff --git a/default.config b/default.config
deleted file mode 100644
index 3ebd4c7..0000000
--- a/default.config
+++ /dev/null
@@ -1,31 +0,0 @@
-[I/O]
-input_format = dir
-input_file_format = plaintext
-input_path = test_files/plaintext
-output_path = test_output
-
-[NLP]
-spacy = en_core_sci_sm
-sentence_splitter = default
-tokenizer = scispacy
-pos_tagger = scispacy
-lemmatizer = scispacy
-chunker = opennlp
-opennlp_host = localhost
-opennlp_port = 8080
-opennlp = /Users/mjsarol/Packages/apache-opennlp-1.9.3
-
-[LEXACCESS]
-host = localhost
-port = 8085
-
-[SERVERS]
-host = ec2-18-223-119-81.us-east-2.compute.amazonaws.com
-metamaplite_port = 12345
-wsd_port = 12346
-gnormplus_port = 12347
-hierarchy_port = 12349
-
-[SEMREP]
-semrules = resources/semrules2020.xml
-ontology_db = resources/12859_2020_3517_MOESM2_ESM.txt
\ No newline at end of file
diff --git a/default.config b/default.config
new file mode 120000
index 0000000..53eea87
--- /dev/null
+++ b/default.config
@@ -0,0 +1 @@
+refactored.config
\ No newline at end of file
diff --git a/lexaccess.py b/lexaccess.py
index 36e6c93..5190871 100644
--- a/lexaccess.py
+++ b/lexaccess.py
@@ -1,4 +1,4 @@
-from jsonrpclib.jsonrpc import ServerProxy
+#from jsonrpclib.jsonrpc import ServerProxy
 import xml.etree.ElementTree as ET
 
 POS_MAPPINGS = {
@@ -74,24 +74,41 @@ class LexRecord:
         #self.infl_vars = record_xml.find('inflVars')
         #self.noun_entry = record_xml.find('nounEntry')
 
+def lexA(s): #could make a parse method in class below
+    import os
+    cs=f'echo {s} | lexAccess -f:id -f:x'
+    s=os.popen(cs).read()
+    return s
+
+import functools
+
 class LexAccess():
     def __init__(self, config):
         host = config['host']
         port = int(config['port'])
-        self.server = ServerProxy("http://%s:%d" % (host, port))
+        #self.server = ServerProxy("http://%s:%d" % (host, port))
 
+    @functools.lru_cache(maxsize=None)
     def lookup(self, text):
-        try:
-            match = self.server.parse(text)
-            tree = ET.ElementTree(ET.fromstring(match.strip()))
-            root = tree.getroot()
-
-            lexrecords_xml = root.findall('lexRecord')
-            if len(lexrecords_xml) > 0:
-                return lexrecords_xml
-        except Exception as e:
-            print(e)
-            print('LexAccess error: ' + e)
+        import re
+        #print(f'lookup:{text}')
+        #ctext=text.replace('(','').replace(')','').replace('>','').replace('<','')
+        ctext= re.sub(r'\W+', ' ', text)
+        ctext= ctext.strip()
+        print(f'lookup:{text}:[{ctext}]')
+        if len(ctext)>0:
+            try:
+                #match = self.server.parse(text)
+                match = lexA(ctext)
+                tree = ET.ElementTree(ET.fromstring(match.strip()))
+                root = tree.getroot()
+
+                lexrecords_xml = root.findall('lexRecord')
+                if len(lexrecords_xml) > 0:
+                    return lexrecords_xml
+            except Exception as e:
+                print(e)
+                print('LexAccess error: ' + e)
         return None
 
     # convert lex records xml to list of lex records object
@@ -143,4 +160,4 @@ class LexAccess():
 
             prev_token_index = token.i
 
-        return matches
\ No newline at end of file
+        return matches
diff --git a/main.py b/main.py
index 9a36606..50a2bf8 100644
--- a/main.py
+++ b/main.py
@@ -84,6 +84,7 @@ def get_concept(term, concepts):
 
 #hierarchy: first mention (e.g. GNormPlus over MetamapLite)
 def referential_analysis(text, ontologies = [GNormPlus, MetamapLite]):
+    #import json
     processes = {}
     with Pool(processes = len(ontologies)) as pool:
         for ontology in ontologies:
@@ -109,6 +110,7 @@ def referential_analysis(text, ontologies = [GNormPlus, MetamapLite]):
     sorted_span_lengths.sort(reverse = True)
 
     merged_annotations = {}
+    t_annotations = {}
     for span_length in sorted_span_lengths:
         for (start, end), concept in span_lengths[span_length].items():
             cur_span_range = set(range(start, end))
@@ -118,10 +120,16 @@ def referential_analysis(text, ontologies = [GNormPlus, MetamapLite]):
                 merged_span_range = set(range(merged_start, merged_end))
                 if len(cur_span_range.intersection(merged_span_range)) > 0:
                     # print(f'DONT ADD: {(start, end)}')
+                    print(f'DONT ADD: {(start, end)}')
                     merge = False
                     break
             if merge:
                 merged_annotations[(start, end)] = concept
+                t_annotations[f't{start}_{end}'] = concept
+
+    with open("an.tmp", 'a') as f:
+        f.write(json.dumps(t_annotations,indent=2)) #error, have2change
+        f.write('\n')
 
     return merged_annotations
 
@@ -286,6 +294,9 @@ def generate_candidates(surface_elements):
                                    'semtype' : concept.semtype})
     return candidates
 
+#@cache
+import functools
+@functools.lru_cache(maxsize=None)
 def lookup(semtype_1, pred_type, semtype_2):
     return '-'.join([semtype_1, pred_type, semtype_2]) in ontology_db
 
@@ -480,6 +491,7 @@ def process_text(text):
     doc = spacynlp(text)
     # print(len(text))
     # print(len(doc))
+    print(f'len:text:{len(text)},doc:{len(doc)}')
     # exit()
 
     sentences = []
@@ -490,7 +502,11 @@ def process_text(text):
         sentence.indicators = annotate_indicators(sentence.spacy, srindicators_list, srindicator_lemmas)
 
         concepts = referential_analysis(text)
-        print(concepts)
+        #print(concepts)
+        print(f'concepts:{concepts}')
+        #with open("an.tmp", 'a') as f:
+        #    f.write(concepts) #error, have2change
+        #    f.write('\n')
     # print('DONE')
         #relational_analysis(sentence.surface_elements)
 
@@ -698,6 +714,7 @@ def process_file(input_file_format, input_file_path, output_file_path = None, mu
     for doc in docs:
         # print('PMID: {}'.format(doc.PMID))
         # print('Title: {}'.format(doc.title))
+        print(f'PMID: {doc.PMID},Title: {doc.title}')
         if doc.title is not None:
             process_text(doc.title)
         process_text(doc.abstract)
@@ -765,4 +782,4 @@ if __name__ == '__main__':
     elif args.input_format == 'file':
         process_file(args.input_file_format, args.input_path, args.output_path)
     else:
-        process_interactive(args.output_path)
\ No newline at end of file
+        process_interactive(args.output_path)
diff --git a/metamaplite.py b/metamaplite.py
index 4b9beff..d767a49 100644
--- a/metamaplite.py
+++ b/metamaplite.py
@@ -6,9 +6,19 @@ class MetamapLite:
         self.port = port
 
     def annotate(self, text):
+        import json
         socket_client = SocketClient(self.host, self.port)
         annotations = socket_client.send(text)
         # print(annotations)
+        print(f'annotate:{type(annotations)}')
+        annotations.replace(';;',';\n;') #1st step to being more comparable
+        print(f'annotate:{annotations}')
+        with open("an.tmp", 'a') as f:
+            f.write(annotations)
+            f.write('\n')
+            #ad=eval(annotations)
+            #annotations2=json.dumps(ad,indent=2)
+            #f.write(annotations2)
         return self.parse_annotations(annotations)
 
     def parse_annotations(self, annotations):
@@ -42,4 +52,4 @@ class MetamapLite:
 
             parsed_annotations[span] = concepts
 
-        return parsed_annotations
\ No newline at end of file
+        return parsed_annotations
diff --git a/server/lexaccess/lexaccess.py b/server/lexaccess/lexaccess.py
index bc243e7..5a1f99f 100755
--- a/server/lexaccess/lexaccess.py
+++ b/server/lexaccess/lexaccess.py
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from jsonrpclib.SimpleJSONRPCServer import SimpleJSONRPCServer
 import optparse
 import os.path
@@ -24,7 +24,8 @@ class LexAccess():
 
         self.process.sendline(text)
         self.process.readline()
-        self.process.expect(u'</lexRecords>')
+        #self.process.expect(u'</lexRecords>')
+        self.process.expect('</lexRecords>')
         # self.process.expect(u'----------')
         
 
@@ -49,7 +50,8 @@ def main():
     #                   help="Path to OpenNLP install [%s]" % DIRECTORY)
 
 
-    path = '/Users/mjsarol/Documents/BioNLP/lexAccess2016/bin/LexAccess -f:id -f:x'
+    #path = '/Users/mjsarol/Documents/BioNLP/lexAccess2016/bin/LexAccess -f:id -f:x'
+    path = 'lexAccess -f:id -f:x'
     # path = 'java -cp /Users/mjsarol/Documents/BioNLP/lexAccess2016/lib/lexAccess2016api.jar:/Users/mjsarol/Documents/BioNLP/lexAccess2016/lib/Other/lexCheck2016api.jar:/Users/mjsarol/Documents/BioNLP/lexAccess2016/lib/jdbcDrivers/HSqlDb/hsqldb.jar:/Users/mjsarol/Documents/BioNLP/lexicon-wrapper Test'
     options, args = parser.parse_args()
 
